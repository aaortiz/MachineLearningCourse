Implement decision tree learning with the 'maxDepth' hyperparameter.

import DecisionTree
model = DecisionTree.DecisionTree()
model.fit(x, y, maxDepth = 5)
yValidatePredicted = model.predict(xValidate)

This is a recursive algorithm that finds the best feature and threshold to split the training data (according to infromation gain)
 then it partitions the data and calls itself recursively.

Recall that Information Gain is:
   
   H(node_data) - sum_i p(feature has i) * H(node_data where feature has i)

And H (entropy) is:
   
   sum_y - p(node_data has label y) * log(p(node_data has label y))

Do not split (and recur) if information gain is 0.

You only need to support numeric input features (as they will work with binary, 0 - 1 features naturally).

To find the splitting threshold for a numeric feature you must:
    1) sort the training data by the feature you are considering (keeping x and y in sync while sorting)
    2) evaluate a potential split between every pair of training examples where the target feature's value changes
        * choose the splitting threshold value half way between the consecutive example's target feature's values
        * e.g. if a features sorted values are [ 1, 1, 4, 4 ] you would consider the threshold of 2.5 [ because that's mid way between 1 and 4 ]


Hand in

2 Points - Your implementation: DecisionTree.py. Make sure it's very easy for the TA to find the critical parts of the code:
            such as the core recursion logic, the maxDepth, the entropy calculation, the split threshold.

4 points - Tune maxDepth with and without numeric features:
        
            featurizer.CreateFeatureSet(xTrainRaw, yTrain, useCategoricalFeatures = True, useNumericFeatures = True)

            Create a short (~3-4 figures and 200 words) writeup of what you learned from the tuning runs. 
            Include all the usual elements (bounds, parameter sweeps, ROC curves). Are the numeric features worth using?